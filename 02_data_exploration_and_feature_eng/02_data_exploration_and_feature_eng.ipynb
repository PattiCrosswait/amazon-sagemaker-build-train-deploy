{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data exploration, preprocessing and feature engineering</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this and the following notebooks we will demonstrate how you can build your ML Pipeline leveraging SKLearn Feature Transformers and SageMaker XGBoost algorithm & after the model is trained, deploy the Pipeline (Feature Transformer and XGBoost) as a SageMaker Inference Pipeline behind a single Endpoint for real-time inference.\n",
    "\n",
    "In particular, in this notebook we will tackle the first steps related to data exploration and preparation. We will use [Amazon Athena](https://aws.amazon.com/athena/) to query our dataset and have a first insight about data quality and available features, [AWS Glue](https://aws.amazon.com/glue/) to create a Data Catalog and [Amazon SageMaker Processing](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) for building the feature transformer model with SKLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "# Check SageMaker Python SDK version\n",
    "import sagemaker\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "def versiontuple(v):\n",
    "    return tuple(map(int, (v.split(\".\"))))\n",
    "\n",
    "if versiontuple(sagemaker.__version__) < versiontuple('2.5.0'):\n",
    "    raise Exception(\"This notebook requires at least SageMaker Python SDK version 2.5.0. Please install it via pip.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install botocore==1.18.3\n",
    "!pip install sagemaker==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu-west-1\n",
      "arn:aws:iam::436518610213:role/service-role/AmazonSageMaker-ExecutionRole-20200922T111694\n",
      "sagemaker-eu-west-1-436518610213\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = 'endtoendmlsm'\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now copy to our bucket the dataset used for this use case. We will use the `windturbine_raw_data_header.csv` made available for this workshop in the `gianpo-public` public S3 bucket. In this Notebook, we will download from that bucket and upload to your bucket so that AWS services can access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "file_key = 'data/raw/windturbine_raw_data_header.csv'\n",
    "copy_source = {\n",
    "    'Bucket': 'gianpo-public',\n",
    "    'Key': 'endtoendml/{0}'.format(file_key)\n",
    "}\n",
    "\n",
    "s3.Bucket(bucket_name).copy(copy_source, '{0}/'.format(prefix) + file_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need now is to infer a schema for our dataset. Thanks to its [integration with AWS Glue](https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html), we will later use Amazon Athena to run SQL queries against our data stored in S3 without the need to import them into a relational database. To do so, Amazon Athena uses the AWS Glue Data Catalog as a central location to store and retrieve table metadata throughout an AWS account. The Athena execution engine, indeed, requires table metadata that instructs it where to read data, how to read it, and other information necessary to process the data.\n",
    "\n",
    "To organize our Glue Data Catalog we create a new database named `endtoendml-db`. To do so, we create a Glue client via Boto and invoke the `create_database` method.\n",
    "\n",
    "However, first we want to make sure these AWS resources to not exist yet to avoid any error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup completed.\n"
     ]
    }
   ],
   "source": [
    "from notebook_utilities import cleanup_glue_resources\n",
    "cleanup_glue_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue')\n",
    "\n",
    "response = glue_client.create_database(DatabaseInput={'Name': 'endtoendml-db'})\n",
    "response = glue_client.get_database(Name='endtoendml-db')\n",
    "response\n",
    "assert response['Database']['Name'] == 'endtoendml-db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a Glue Crawler that we point to the S3 path where the dataset resides, and the crawler creates table definitions in the Data Catalog.\n",
    "To grant the correct set of access permission to the crawler, we use one of the roles created before (`GlueServiceRole-endtoendml`) whose policy grants AWS Glue access to data stored in your S3 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue_client.create_crawler(\n",
    "    Name='endtoendml-crawler',\n",
    "    Role='service-role/GlueServiceRole-endtoendml', \n",
    "    DatabaseName='endtoendml-db',\n",
    "    Targets={'S3Targets': [{'Path': '{0}/{1}/data/raw/'.format(bucket_name, prefix)}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to run the crawler with the `start_crawler` API and to monitor its status upon completion through the `get_crawler_metrics` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n"
     ]
    }
   ],
   "source": [
    "glue_client.start_crawler(Name='endtoendml-crawler')\n",
    "\n",
    "while glue_client.get_crawler_metrics(CrawlerNameList=['endtoendml-crawler'])['CrawlerMetricsList'][0]['TablesCreated'] == 0:\n",
    "    print('RUNNING')\n",
    "    time.sleep(15)\n",
    "    \n",
    "assert glue_client.get_crawler_metrics(CrawlerNameList=['endtoendml-crawler'])['CrawlerMetricsList'][0]['TablesCreated'] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the crawler has finished its job, we can retrieve the Table definition for the newly created table.\n",
    "As you can see, the crawler has been able to correctly identify 12 fields, infer a type for each column and assign a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table': {'Name': 'raw',\n",
       "  'DatabaseName': 'endtoendml-db',\n",
       "  'Owner': 'owner',\n",
       "  'CreateTime': datetime.datetime(2020, 11, 3, 11, 39, 33, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2020, 11, 3, 11, 39, 33, tzinfo=tzlocal()),\n",
       "  'LastAccessTime': datetime.datetime(2020, 11, 3, 11, 39, 33, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'turbine_id', 'Type': 'string'},\n",
       "    {'Name': 'turbine_type', 'Type': 'string'},\n",
       "    {'Name': 'wind_speed', 'Type': 'bigint'},\n",
       "    {'Name': 'rpm_blade', 'Type': 'bigint'},\n",
       "    {'Name': 'oil_temperature', 'Type': 'double'},\n",
       "    {'Name': 'oil_level', 'Type': 'bigint'},\n",
       "    {'Name': 'temperature', 'Type': 'bigint'},\n",
       "    {'Name': 'humidity', 'Type': 'bigint'},\n",
       "    {'Name': 'vibrations_frequency', 'Type': 'bigint'},\n",
       "    {'Name': 'pressure', 'Type': 'bigint'},\n",
       "    {'Name': 'wind_direction', 'Type': 'string'},\n",
       "    {'Name': 'breakdown', 'Type': 'string'}],\n",
       "   'Location': 's3://sagemaker-eu-west-1-436518610213/endtoendmlsm/data/raw/',\n",
       "   'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
       "   'Compressed': False,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n",
       "    'Parameters': {'field.delim': ','}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'CrawlerSchemaSerializerVersion': '1.0',\n",
       "    'UPDATED_BY_CRAWLER': 'endtoendml-crawler',\n",
       "    'areColumnsQuoted': 'false',\n",
       "    'averageRecordSize': '75',\n",
       "    'classification': 'csv',\n",
       "    'columnsOrdered': 'true',\n",
       "    'compressionType': 'none',\n",
       "    'delimiter': ',',\n",
       "    'objectCount': '1',\n",
       "    'recordCount': '564265',\n",
       "    'sizeKey': '42319941',\n",
       "    'skip.header.line.count': '1',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'CrawlerSchemaSerializerVersion': '1.0',\n",
       "   'UPDATED_BY_CRAWLER': 'endtoendml-crawler',\n",
       "   'areColumnsQuoted': 'false',\n",
       "   'averageRecordSize': '75',\n",
       "   'classification': 'csv',\n",
       "   'columnsOrdered': 'true',\n",
       "   'compressionType': 'none',\n",
       "   'delimiter': ',',\n",
       "   'objectCount': '1',\n",
       "   'recordCount': '564265',\n",
       "   'sizeKey': '42319941',\n",
       "   'skip.header.line.count': '1',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:sts::436518610213:assumed-role/GlueServiceRole-endtoendml/AWS-Crawler',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '436518610213'},\n",
       " 'ResponseMetadata': {'RequestId': 'ad532d28-ca26-4fb6-adb5-409e440e5c8d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Tue, 03 Nov 2020 11:39:48 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2093',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'ad532d28-ca26-4fb6-adb5-409e440e5c8d'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = glue_client.get_table(DatabaseName='endtoendml-db', Name='raw')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our knowledge of the dataset, we can be more specific with column names and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '49b04c8d-110c-4aab-8527-b41c27f25027',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Tue, 03 Nov 2020 11:39:48 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '49b04c8d-110c-4aab-8527-b41c27f25027'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have to remove the CatalogId key from the dictionary due to an breaking change\n",
    "# intrduced in botocore 1.17.18.\n",
    "del table['Table']['CatalogId']\n",
    "\n",
    "table['Table']['StorageDescriptor']['Columns'] = [{'Name': 'turbine_id', 'Type': 'string'},\n",
    "                                                  {'Name': 'turbine_type', 'Type': 'string'},\n",
    "                                                  {'Name': 'wind_speed', 'Type': 'double'},\n",
    "                                                  {'Name': 'rpm_blade', 'Type': 'double'},\n",
    "                                                  {'Name': 'oil_temperature', 'Type': 'double'},\n",
    "                                                  {'Name': 'oil_level', 'Type': 'double'},\n",
    "                                                  {'Name': 'temperature', 'Type': 'double'},\n",
    "                                                  {'Name': 'humidity', 'Type': 'double'},\n",
    "                                                  {'Name': 'vibrations_frequency', 'Type': 'double'},\n",
    "                                                  {'Name': 'pressure', 'Type': 'double'},\n",
    "                                                  {'Name': 'wind_direction', 'Type': 'string'},\n",
    "                                                  {'Name': 'breakdown', 'Type': 'string'}]\n",
    "updated_table = table['Table']\n",
    "updated_table.pop('DatabaseName', None)\n",
    "updated_table.pop('CreateTime', None)\n",
    "updated_table.pop('UpdateTime', None)\n",
    "updated_table.pop('CreatedBy', None)\n",
    "updated_table.pop('IsRegisteredWithLakeFormation', None)\n",
    "\n",
    "glue_client.update_table(\n",
    "    DatabaseName='endtoendml-db',\n",
    "    TableInput=updated_table\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data exploration with Amazon Athena</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data exploration, let's install AWS Data wrangler, a open source python initiative that extends the power of Pandas library to AWS connecting DataFrames and AWS data related services (Amazon Redshift, AWS Glue, Amazon Athena, Amazon EMR, Amazon QuickSight, etc).  please visit: https://aws-data-wrangler.readthedocs.io/en/stable/what.html for additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install awswrangler\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  turbine_id turbine_type  wind_speed  rpm_blade  oil_temperature  oil_level  \\\n",
      "0     TID003         HAWT        80.0       61.0              NaN       34.0   \n",
      "1     TID010         HAWT        85.0       78.0             36.0       28.0   \n",
      "2     TID007         HAWT        47.0       31.0             31.0       23.0   \n",
      "3     TID008         VAWT        73.0       70.0             38.0        8.0   \n",
      "4     TID003         HAWT        16.0       23.0             46.0        9.0   \n",
      "5     TID001         HAWT        78.0       71.0             30.0       11.0   \n",
      "6     TID009         HAWT        80.0       25.0             37.0       31.0   \n",
      "7     TID002         VAWT        59.0       29.0             37.0       10.0   \n",
      "\n",
      "   temperature  humidity  vibrations_frequency  pressure wind_direction  \\\n",
      "0         33.0      26.0                   1.0      77.0              E   \n",
      "1         35.0      43.0                  15.0      62.0             NE   \n",
      "2         46.0      62.0                  15.0      32.0              N   \n",
      "3         17.0      66.0                   6.0      80.0             SW   \n",
      "4         76.0      53.0                  14.0      29.0              W   \n",
      "5         66.0      79.0                   1.0      81.0             SW   \n",
      "6         40.0      75.0                   4.0      56.0             NW   \n",
      "7         25.0      83.0                  13.0      55.0             SE   \n",
      "\n",
      "  breakdown  \n",
      "0        no  \n",
      "1       yes  \n",
      "2        no  \n",
      "3       yes  \n",
      "4        no  \n",
      "5        no  \n",
      "6        no  \n",
      "7        no  \n"
     ]
    }
   ],
   "source": [
    "dfs = (wr.athena.read_sql_query(\"SELECT * FROM raw limit 8\", database=\"endtoendml-db\", ctas_approach=False))\n",
    "print(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another SQL query to count how many records we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     _col0\n",
      "0  1000000\n"
     ]
    }
   ],
   "source": [
    "dfs = (wr.athena.read_sql_query(\"SELECT COUNT(*) FROM raw \", database=\"endtoendml-db\", ctas_approach=False))\n",
    "print(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to see what are possible values for the field \"breakdown\" and how frequently they occur over the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  breakdown  percent\n",
      "0       yes  13.6579\n",
      "1        no  86.3421\n"
     ]
    }
   ],
   "source": [
    "dfs = (wr.athena.read_sql_query(\"SELECT breakdown, (COUNT(breakdown) * 100.0 / (SELECT COUNT(*) FROM raw )) AS percent FROM raw GROUP BY breakdown\", database=\"endtoendml-db\", ctas_approach=False))\n",
    "print(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  breakdown  bd_count\n",
      "0       yes    136579\n",
      "1        no    863421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfs = (wr.athena.read_sql_query(\"SELECT breakdown, COUNT(breakdown) AS bd_count FROM raw GROUP BY breakdown\", database=\"endtoendml-db\", ctas_approach=False))\n",
    "print(dfs)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(dfs.breakdown, dfs.bd_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have discovered that the dataset is quite unbalanced, although we are not going to try balancing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  turbine_type\n",
      "0         <NA>\n",
      "1         VAWT\n",
      "2         HAWT\n"
     ]
    }
   ],
   "source": [
    "dfs = (wr.athena.read_sql_query(\"SELECT  DISTINCT(turbine_type) FROM raw \", database=\"endtoendml-db\", ctas_approach=False))\n",
    "print(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   _col0\n",
      "0  38297\n"
     ]
    }
   ],
   "source": [
    "dfs = (wr.athena.read_sql_query(\"SELECT  COUNT(*) FROM raw WHERE oil_temperature IS NULL GROUP BY oil_temperature \", database=\"endtoendml-db\", ctas_approach=False))\n",
    "print(dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also realized there are a few null values that need to be managed during the data preparation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of keeping the data exploration step short during the workshop, we are not going to execute additional queries. However, feel free to explore the dataset more if you have time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: you can go to Amazon Athena console and check for query duration under History tab: usually queries are executed in a few seconds, then it some time for Pandas to load results into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started with preprocessing and feature engineering, we want to leverage on Amazon SageMaker Experiments to track the experimentations that we will be executing.\n",
    "We are going to create a new experiment and then a new trial, that represents a multi-step ML workflow (e.g. preprocessing stage1, preprocessing stage2, training stage, etc.). Each step of a trial maps to a trial component in SageMaker Experiments.\n",
    "\n",
    "We will use the Amazon SageMaker Experiments SDK to interact with the service from the notebooks. Additional info and documentation is available here: https://github.com/aws/sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.7/site-packages (0.1.24)\n",
      "Requirement already satisfied: boto3>=1.12.8 in /opt/conda/lib/python3.7/site-packages (from sagemaker-experiments) (1.15.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.12.8->sagemaker-experiments) (0.10.0)\n",
      "Collecting botocore<1.19.0,>=1.18.3\n",
      "  Using cached botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.12.8->sagemaker-experiments) (0.3.3)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.0,>=1.18.3->boto3>=1.12.8->sagemaker-experiments) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.0,>=1.18.3->boto3>=1.12.8->sagemaker-experiments) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.0,>=1.18.3->boto3>=1.12.8->sagemaker-experiments) (1.14.0)\n",
      "\u001b[31mERROR: awscli 1.18.144 has requirement botocore==1.18.3, but you'll have botocore 1.18.18 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: aiobotocore 1.1.1 has requirement botocore<1.17.45,>=1.17.44, but you'll have botocore 1.18.18 which is incompatible.\u001b[0m\n",
      "Installing collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.17.44\n",
      "    Uninstalling botocore-1.17.44:\n",
      "      Successfully uninstalled botocore-1.17.44\n",
      "Successfully installed botocore-1.18.18\n"
     ]
    }
   ],
   "source": [
    "pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are creating the experiment, or loading if it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end-to-end-ml-sagemaker-1604405983\n",
      "Experiment created.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments import experiment\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "experiment_name = 'end-to-end-ml-sagemaker-{0}'.format(str(int(time.time())))\n",
    "print(experiment_name)\n",
    "current_experiment = None\n",
    "\n",
    "try:\n",
    "    current_experiment = experiment.Experiment.load(experiment_name)\n",
    "    print('Experiment loaded.')\n",
    "except ClientError as ex:\n",
    "    if ex.response['Error']['Code'] == 'ResourceNotFound':\n",
    "        # Create experiment\n",
    "        current_experiment = experiment.Experiment.create(experiment_name=experiment_name,\n",
    "                                                          description='SageMaker workshop experiment')\n",
    "        print('Experiment created.')\n",
    "    else:\n",
    "        raise ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our experiment, we can create a new trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "trial_name = 'sklearn-xgboost-{0}'.format(str(int(time.time())))\n",
    "current_trial = current_experiment.create_trial(trial_name=trial_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now own, we will use the experiment and the trial as configuration parameters for the preprocessing and training jobs, to make sure we track executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'experiment_name' (str)\n",
      "Stored 'trial_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store experiment_name\n",
    "%store trial_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing and Feature Engineering with Amazon SageMaker Processing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing and feature engineering code is implemented in the `source_dir/preprocessor.py` file.\n",
    "\n",
    "You can go through the code and see that a few categorical columns required one-hot encoding, plus we are filling some NaN values based on domain knowledge.\n",
    "Once the SKLearn fit() and transform() is done, we are splitting our dataset into 80/20 train & validation and then saving to the output paths whose content will be automatically uploaded to Amazon S3 by SageMaker Processing. Finally, we also save the featurizer model as it will be reused later for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mwarnings\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "subprocess.call([\u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-experiments\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msmexperiments\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtracker\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Tracker\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mexternals\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m joblib\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m OneHotEncoder, LabelEncoder\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcompose\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m make_column_transformer\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mexceptions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataConversionWarning\n",
      "warnings.filterwarnings(action=\u001b[33m'\u001b[39;49;00m\u001b[33mignore\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, category=DataConversionWarning)\n",
      "\n",
      "columns = [\u001b[33m'\u001b[39;49;00m\u001b[33mturbine_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mturbine_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwind_speed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mrpm_blade\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33moil_temperature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "           \u001b[33m'\u001b[39;49;00m\u001b[33moil_level\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtemperature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mhumidity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mvibrations_frequency\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpressure\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwind_direction\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mbreakdown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m==\u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[37m# Read the arguments passed to the script.\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-test-split-ratio\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.3\u001b[39;49;00m)\n",
      "    args, _ = parser.parse_known_args()\n",
      "    \n",
      "    \u001b[37m# Tracking specific parameter value during job.\u001b[39;49;00m\n",
      "    tracker = Tracker.load()\n",
      "    tracker.log_parameter(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain-test-split-ratio\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.train_test_split_ratio)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mReceived arguments \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args))\n",
      "\n",
      "    \u001b[37m# Read input data into a Pandas dataframe.\u001b[39;49;00m\n",
      "    input_data_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwindturbine_raw_data_header.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mReading input data from \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(input_data_path))\n",
      "    df = pd.read_csv(input_data_path)\n",
      "    df.columns = columns\n",
      "    \n",
      "    \u001b[37m# Replacing certain null values.\u001b[39;49;00m\n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33mturbine_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = df[\u001b[33m'\u001b[39;49;00m\u001b[33mturbine_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].fillna(\u001b[33m\"\u001b[39;49;00m\u001b[33mHAWT\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    tracker.log_parameter(\u001b[33m'\u001b[39;49;00m\u001b[33mdefault-turbine-type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mHAWT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    df[\u001b[33m'\u001b[39;49;00m\u001b[33moil_temperature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = df[\u001b[33m'\u001b[39;49;00m\u001b[33moil_temperature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].fillna(\u001b[34m37.0\u001b[39;49;00m)\n",
      "    tracker.log_parameter(\u001b[33m'\u001b[39;49;00m\u001b[33mdefault-oil-temperature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m37.0\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Defining one-hot encoders.\u001b[39;49;00m\n",
      "    transformer = make_column_transformer(\n",
      "        ([\u001b[33m'\u001b[39;49;00m\u001b[33mturbine_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mturbine_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwind_direction\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], OneHotEncoder(sparse=\u001b[34mFalse\u001b[39;49;00m)), remainder=\u001b[33m\"\u001b[39;49;00m\u001b[33mpassthrough\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    \n",
      "    X = df.drop(\u001b[33m'\u001b[39;49;00m\u001b[33mbreakdown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis=\u001b[34m1\u001b[39;49;00m)\n",
      "    y = df[\u001b[33m'\u001b[39;49;00m\u001b[33mbreakdown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \n",
      "    featurizer_model = transformer.fit(X)\n",
      "    features = featurizer_model.transform(X)\n",
      "    labels = LabelEncoder().fit_transform(y)\n",
      "    \n",
      "    \u001b[37m# Splitting.\u001b[39;49;00m\n",
      "    split_ratio = args.train_test_split_ratio\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSplitting data into train and validation sets with ratio \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(split_ratio))\n",
      "    X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=split_ratio, random_state=\u001b[34m0\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain features shape after preprocessing: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(X_train.shape))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mValidation features shape after preprocessing: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(X_val.shape))\n",
      "    \n",
      "    \u001b[37m# Saving outputs.\u001b[39;49;00m\n",
      "    train_features_output_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain_features.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    train_labels_output_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain_labels.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    val_features_output_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/val\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mval_features.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    val_labels_output_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/val\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mval_labels.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving training features to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_features_output_path))\n",
      "    pd.DataFrame(X_train).to_csv(train_features_output_path, header=\u001b[34mFalse\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving validation features to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(val_features_output_path))\n",
      "    pd.DataFrame(X_val).to_csv(val_features_output_path, header=\u001b[34mFalse\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving training labels to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_labels_output_path))\n",
      "    pd.DataFrame(y_train).to_csv(train_labels_output_path, header=\u001b[34mFalse\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving validation labels to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(val_labels_output_path))\n",
      "    pd.DataFrame(y_val).to_csv(val_labels_output_path, header=\u001b[34mFalse\u001b[39;49;00m, index=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Saving model.\u001b[39;49;00m\n",
      "    model_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.joblib\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    model_output_path = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.tar.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving featurizer model to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model_output_path))\n",
      "    joblib.dump(featurizer_model, model_path)\n",
      "    tar = tarfile.open(model_output_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw:gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    tar.add(model_path, arcname=\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.joblib\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    tar.close()\n",
      "    \n",
      "    tracker.close()\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source_dir/preprocessor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring an Amazon SageMaker Processing job through the SM Python SDK requires to create a `Processor` object (in this case `SKLearnProcessor` as we are using the default SKLearn container for processing); we can specify how many instances we are going to use and what instance type is requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(role=role,\n",
    "                                     base_job_name='end-to-end-ml-sm-proc',\n",
    "                                     instance_type='ml.m5.large',\n",
    "                                     instance_count=1,\n",
    "                                     framework_version='0.20.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can invoke the `run()` method of the `Processor` object to kick-off the job, specifying the script to execute, its arguments and the configuration of inputs and outputs as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name end-to-end-ml-sm-proc-2020-11-03-12-20-04-365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  end-to-end-ml-sm-proc-2020-11-03-12-20-04-365\n",
      "Inputs:  [{'InputName': 'raw_data', 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-1-436518610213/endtoendmlsm/data/raw/', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-1-436518610213/end-to-end-ml-sm-proc-2020-11-03-12-20-04-365/input/code/preprocessor.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train_data', 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-1-436518610213/endtoendmlsm/data/preprocessed/train/', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'val_data', 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-1-436518610213/endtoendmlsm/data/preprocessed/val/', 'LocalPath': '/opt/ml/processing/val', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'model', 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-1-436518610213/endtoendmlsm/output/sklearn/', 'LocalPath': '/opt/ml/processing/model', 'S3UploadMode': 'EndOfJob'}}]\n",
      "....................\u001b[34mCollecting sagemaker-experiments\n",
      "  Downloading https://files.pythonhosted.org/packages/62/60/f8c7d6cc88866374913c85d4d872b67f019cfbbdd051980bd7b4cc4e546d/sagemaker_experiments-0.1.24-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mCollecting boto3>=1.12.8\n",
      "  Downloading https://files.pythonhosted.org/packages/95/b5/78c837d962dd2b629ac551647c61537c56fe8f669d9e9efbed1abd798e65/boto3-1.16.10.tar.gz (97kB)\u001b[0m\n",
      "\u001b[34mCollecting botocore<1.20.0,>=1.19.10\n",
      "  Downloading https://files.pythonhosted.org/packages/50/11/765dce0f69eb6f6db6a189e1848f553575a0189b24bd059eaa24fd9e003d/botocore-1.19.10-py2.py3-none-any.whl (6.7MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /miniconda3/lib/python3.7/site-packages (from boto3>=1.12.8->sagemaker-experiments) (0.9.4)\u001b[0m\n",
      "\u001b[34mCollecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\u001b[0m\n",
      "\u001b[34mCollecting urllib3<1.26,>=1.25.4; python_version != \"3.4\"\n",
      "  Downloading https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /miniconda3/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.10->boto3>=1.12.8->sagemaker-experiments) (2.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /miniconda3/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.10->boto3>=1.12.8->sagemaker-experiments) (1.12.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: boto3\n",
      "  Building wheel for boto3 (setup.py): started\n",
      "  Building wheel for boto3 (setup.py): finished with status 'done'\n",
      "  Created wheel for boto3: filename=boto3-1.16.10-py2.py3-none-any.whl size=128450 sha256=3a5c671cb6a97bfb28c1c110b4a76b4bf08318ff6ef37fe071e458cf5af6ae13\n",
      "  Stored in directory: /root/.cache/pip/wheels/36/dd/27/cf4486e326eeb099cbe273be73945d394f66914c0f14ac3b09\u001b[0m\n",
      "\u001b[34mSuccessfully built boto3\u001b[0m\n",
      "\u001b[34mInstalling collected packages: urllib3, botocore, s3transfer, boto3, sagemaker-experiments\n",
      "  Found existing installation: urllib3 1.24.2\n",
      "    Uninstalling urllib3-1.24.2:\n",
      "      Successfully uninstalled urllib3-1.24.2\n",
      "  Found existing installation: botocore 1.13.6\n",
      "    Uninstalling botocore-1.13.6:\n",
      "      Successfully uninstalled botocore-1.13.6\n",
      "  Found existing installation: s3transfer 0.2.1\n",
      "    Uninstalling s3transfer-0.2.1:\n",
      "      Successfully uninstalled s3transfer-0.2.1\n",
      "  Found existing installation: boto3 1.10.6\n",
      "    Uninstalling boto3-1.10.6:\n",
      "      Successfully uninstalled boto3-1.10.6\u001b[0m\n",
      "\u001b[34mSuccessfully installed boto3-1.16.10 botocore-1.19.10 s3transfer-0.3.3 sagemaker-experiments-0.1.24 urllib3-1.25.11\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mReceived arguments Namespace(train_test_split_ratio=0.2)\u001b[0m\n",
      "\u001b[34mReading input data from /opt/ml/processing/input/windturbine_raw_data_header.csv\u001b[0m\n",
      "\u001b[34mSplitting data into train and validation sets with ratio 0.2\u001b[0m\n",
      "\u001b[34mTrain features shape after preprocessing: (800000, 28)\u001b[0m\n",
      "\u001b[34mValidation features shape after preprocessing: (200000, 28)\u001b[0m\n",
      "\u001b[34mSaving training features to /opt/ml/processing/train/train_features.csv\u001b[0m\n",
      "\u001b[34mSaving validation features to /opt/ml/processing/val/val_features.csv\u001b[0m\n",
      "\u001b[34mSaving training labels to /opt/ml/processing/train/train_labels.csv\u001b[0m\n",
      "\u001b[34mSaving validation labels to /opt/ml/processing/val/val_labels.csv\u001b[0m\n",
      "\u001b[34mSaving featurizer model to /opt/ml/processing/model/model.tar.gz\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data_path = 's3://{0}/{1}/data/raw/'.format(bucket_name, prefix)\n",
    "train_data_path = 's3://{0}/{1}/data/preprocessed/train/'.format(bucket_name, prefix)\n",
    "val_data_path = 's3://{0}/{1}/data/preprocessed/val/'.format(bucket_name, prefix)\n",
    "model_path = 's3://{0}/{1}/output/sklearn/'.format(bucket_name, prefix)\n",
    "\n",
    "# Experiment tracking configuration\n",
    "experiment_config={\n",
    "    \"ExperimentName\": current_experiment.experiment_name,\n",
    "    \"TrialName\": current_trial.trial_name,\n",
    "    \"TrialComponentDisplayName\": \"sklearn-preprocessing\",\n",
    "}\n",
    "\n",
    "sklearn_processor.run(code='source_dir/preprocessor.py',\n",
    "                      inputs=[ProcessingInput(input_name='raw_data', source=raw_data_path, destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data', source='/opt/ml/processing/train', destination=train_data_path),\n",
    "                               ProcessingOutput(output_name='val_data', source='/opt/ml/processing/val', destination=val_data_path),\n",
    "                               ProcessingOutput(output_name='model', source='/opt/ml/processing/model', destination=model_path)],\n",
    "                      arguments=['--train-test-split-ratio', '0.2'],\n",
    "                      experiment_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job is completed, we can give a look at the preprocessed dataset, by loading the validation features as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'val_features.csv'\n",
    "s3_key_prefix = '{0}/data/preprocessed/val/{1}'.format(prefix, file_name)\n",
    "\n",
    "sagemaker_session.download_data('./', bucket_name, s3_key_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         \u001b[0mmax_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.max_rows\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0mmin_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.min_rows\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self, buf, encoding, classes, notebook, border)\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mincluded\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mopening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefault\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m    985\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTMLFormatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotebookFormatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/formats/html.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, formatter, classes, border)\u001b[0m\n\u001b[1;32m     57\u001b[0m         self.col_space = {\n\u001b[1;32m     58\u001b[0m             \u001b[0mcolumn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{value}px\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         }\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   0.0  0.0.1  0.0.2  0.0.3  1.0  0.0.4  0.0.5  0.0.6  0.0.7  0.0.8  ...  \\\n",
       "0  0.0    1.0    0.0    0.0  0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "1  0.0    0.0    0.0    0.0  1.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "2  0.0    0.0    0.0    0.0  0.0    0.0    0.0    1.0    0.0    0.0  ...   \n",
       "3  0.0    0.0    0.0    0.0  0.0    0.0    0.0    1.0    0.0    0.0  ...   \n",
       "4  0.0    0.0    0.0    0.0  0.0    0.0    0.0    0.0    1.0    0.0  ...   \n",
       "5  0.0    0.0    0.0    0.0  0.0    0.0    1.0    0.0    0.0    0.0  ...   \n",
       "6  0.0    0.0    1.0    0.0  0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "7  0.0    0.0    0.0    0.0  0.0    0.0    0.0    0.0    1.0    0.0  ...   \n",
       "8  0.0    0.0    0.0    0.0  1.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "9  0.0    0.0    0.0    0.0  0.0    0.0    0.0    0.0    0.0    1.0  ...   \n",
       "\n",
       "   0.0.15  0.0.16  25.0  30.0  30.0.1  17.0  38.0  16.0  1.0.3  20.0  \n",
       "0     0.0     0.0  37.0  43.0    41.0  26.0  65.0  71.0   10.0  76.0  \n",
       "1     0.0     0.0  22.0  35.0    35.0  22.0  85.0  52.0    3.0  22.0  \n",
       "2     0.0     0.0  66.0  41.0    37.0  31.0  37.0  59.0    2.0  30.0  \n",
       "3     0.0     0.0  19.0  48.0    49.0   7.0  84.0  38.0    3.0  51.0  \n",
       "4     0.0     0.0  65.0  65.0    48.0  17.0  27.0  33.0    2.0  40.0  \n",
       "5     1.0     0.0  63.0  26.0    26.0  10.0  50.0  78.0   13.0  59.0  \n",
       "6     0.0     0.0  79.0  46.0    35.0  11.0  40.0  38.0    2.0  59.0  \n",
       "7     0.0     0.0  16.0  48.0    45.0  16.0  17.0  21.0    2.0  49.0  \n",
       "8     0.0     0.0  83.0  53.0    38.0  20.0  81.0  46.0    5.0  62.0  \n",
       "9     0.0     0.0  38.0  83.0    30.0  35.0  43.0  77.0    4.0  34.0  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the categorical variables have been one-hot encoded, and you are free to check that we do not have NaN values anymore as expected.\n",
    "Note that exploring the dataset locally with Pandas vs using Amazon Athena is possible given the limited size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment analytics\n",
    "\n",
    "You can visualize experiment analytics either from Amazon SageMaker Studio Experiments plug-in or using the SDK from a notebook, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         \u001b[0mmax_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.max_rows\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0mmin_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.min_rows\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self, buf, encoding, classes, notebook, border)\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mincluded\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mopening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefault\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m    985\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTMLFormatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotebookFormatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/formats/html.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, formatter, classes, border)\u001b[0m\n\u001b[1;32m     57\u001b[0m         self.col_space = {\n\u001b[1;32m     58\u001b[0m             \u001b[0mcolumn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{value}px\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         }\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                  TrialComponentName            DisplayName  \\\n",
       "0  end-to-end-ml-sm-proc-2020-11-03-12-20-04-365-...  sklearn-preprocessing   \n",
       "\n",
       "                                           SourceArn  SageMaker.InstanceCount  \\\n",
       "0  arn:aws:sagemaker:eu-west-1:436518610213:proce...                      1.0   \n",
       "\n",
       "  SageMaker.InstanceType  SageMaker.VolumeSizeInGB  default-oil-temperature  \\\n",
       "0            ml.m5.large                      30.0                     37.0   \n",
       "\n",
       "  default-turbine-type  train-test-split-ratio SageMaker.ImageUri - MediaType  \\\n",
       "0                 HAWT                     0.2                           None   \n",
       "\n",
       "   ... code - MediaType                                       code - Value  \\\n",
       "0  ...             None  s3://sagemaker-eu-west-1-436518610213/end-to-e...   \n",
       "\n",
       "  raw_data - MediaType                                   raw_data - Value  \\\n",
       "0                 None  s3://sagemaker-eu-west-1-436518610213/endtoend...   \n",
       "\n",
       "  model - MediaType                                      model - Value  \\\n",
       "0              None  s3://sagemaker-eu-west-1-436518610213/endtoend...   \n",
       "\n",
       "  train_data - MediaType                                 train_data - Value  \\\n",
       "0                   None  s3://sagemaker-eu-west-1-436518610213/endtoend...   \n",
       "\n",
       "  val_data - MediaType                                   val_data - Value  \n",
       "0                 None  s3://sagemaker-eu-west-1-436518610213/endtoend...  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "analytics = ExperimentAnalytics(experiment_name=experiment_name)\n",
    "analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the preprocessing and feature engineering are completed, you can move to the next notebook in the **03_train_model** folder to start model training."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
